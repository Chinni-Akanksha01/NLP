{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUAKQnSurZwGXa13ZRxXCj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chinni-Akanksha01/NLP/blob/main/NLP_ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the spellchecker library\n",
        "!pip install pyspellchecker\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "def correct_query(query):\n",
        "    spell = SpellChecker()\n",
        "\n",
        "    # Add custom words to prioritize certain corrections\n",
        "    spell.word_frequency.load_words(['weather', 'tomorrow', 'new york'])\n",
        "\n",
        "    words = query.split()\n",
        "\n",
        "    # Correct each word if it's misspelled\n",
        "    corrected_words = []\n",
        "    for word in words:\n",
        "        if word.lower() == \"wether\":  # Explicit rule for this common mistake\n",
        "            corrected_words.append(\"weather\")\n",
        "        else:\n",
        "            corrected_words.append(spell.correction(word) if word in spell.unknown(words) else word)\n",
        "\n",
        "    # Join the corrected words and ensure proper capitalization\n",
        "    corrected_query = ' '.join(corrected_words).capitalize()\n",
        "    return corrected_query\n",
        "\n",
        "query = \"wether tommorrow in new yrok\"\n",
        "corrected_query = correct_query(query)\n",
        "print(corrected_query)\n",
        "# Output: \"Weather tomorrow in new york\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm9mJcwJLSoR",
        "outputId": "9658190f-c698-4d05-e44f-8e4f6823d7a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.2)\n",
            "Weather tomorrow in new york\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Dictionary of known words for segmentation\n",
        "DICTIONARY = set([\n",
        "    \"best\", \"day\", \"ever\", \"get\", \"started\", \"now\", \"example\", \"com\",\n",
        "    \"deterministic\", \"url\", \"hashtag\", \"segmentation\"\n",
        "])\n",
        "\n",
        "def segment_text(text, dictionary):\n",
        "    \"\"\"\n",
        "    Segments text deterministically using a dictionary of words.\n",
        "    \"\"\"\n",
        "    text = text.lower()  # Case normalization\n",
        "    segments = []\n",
        "    while text:\n",
        "        match_found = False\n",
        "        for i in range(len(text), 0, -1):\n",
        "            prefix = text[:i]\n",
        "            if prefix in dictionary:\n",
        "                segments.append(prefix)\n",
        "                text = text[i:]\n",
        "                match_found = True\n",
        "                break\n",
        "        if not match_found:  # If no match, consider the first character as a word\n",
        "            segments.append(text[0])\n",
        "            text = text[1:]\n",
        "    return segments\n",
        "\n",
        "def split_hashtag_or_url(input_string):\n",
        "    \"\"\"\n",
        "    Splits hashtags or URLs deterministically.\n",
        "    \"\"\"\n",
        "    # Remove special characters for hashtags\n",
        "    if input_string.startswith('#'):\n",
        "        input_string = input_string[1:]\n",
        "\n",
        "    # For URLs, strip the protocol\n",
        "    if input_string.startswith(('http://', 'https://')):\n",
        "        input_string = re.sub(r'^https?://', '', input_string)\n",
        "\n",
        "    # Split based on non-alphanumeric characters\n",
        "    parts = re.split(r'[\\W_]', input_string)\n",
        "    segments = []\n",
        "    for part in parts:\n",
        "        if part:  # Skip empty parts\n",
        "            segments.extend(segment_text(part, DICTIONARY))\n",
        "    return segments\n",
        "\n",
        "# Examples\n",
        "hashtag = \"#BestDayEver\"\n",
        "url = \"http://example.com/GetStartedNow\"\n",
        "\n",
        "print(\"Hashtag Segmentation:\", split_hashtag_or_url(hashtag))\n",
        "print(\"URL Segmentation:\", split_hashtag_or_url(url))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDtDGQyjLbBE",
        "outputId": "279e45bf-ccfd-4c05-c73f-6024486adff9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashtag Segmentation: ['best', 'day', 'ever']\n",
            "URL Segmentation: ['example', 'com', 'get', 'started', 'now']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def get_sense(word, context):\n",
        "    \"\"\"\n",
        "    Disambiguates the sense of a word based on its context using WordNet.\n",
        "    \"\"\"\n",
        "    # Tokenize the context\n",
        "    context_tokens = set(word_tokenize(context.lower()))\n",
        "\n",
        "    # Get all synsets (senses) for the word\n",
        "    synsets = wn.synsets(word)\n",
        "\n",
        "    if not synsets:\n",
        "        return None  # No senses found in WordNet\n",
        "\n",
        "    best_sense = None\n",
        "    max_overlap = 0\n",
        "\n",
        "    for synset in synsets:\n",
        "        # Get the definition, examples, and related terms for the synset\n",
        "        definition = synset.definition()\n",
        "        examples = synset.examples()\n",
        "        related_terms = set()\n",
        "\n",
        "        # Include synonyms and hypernyms for better matching\n",
        "        related_terms.update(lemma.name() for lemma in synset.lemmas())\n",
        "        related_terms.update(hypernym.name().split('.')[0] for hypernym in synset.hypernyms())\n",
        "\n",
        "        # Combine definition, examples, and related terms\n",
        "        sense_words = set(word_tokenize(definition.lower()))\n",
        "        for example in examples:\n",
        "            sense_words.update(word_tokenize(example.lower()))\n",
        "        sense_words.update(related_terms)\n",
        "\n",
        "        # Calculate overlap between sense words and context tokens\n",
        "        overlap = len(sense_words & context_tokens)\n",
        "        if overlap > max_overlap:\n",
        "            max_overlap = overlap\n",
        "            best_sense = synset\n",
        "\n",
        "    return best_sense\n",
        "\n",
        "# Example usage\n",
        "word = \"mouse\"\n",
        "context1 = \"The mouse ran across the field to avoid the cat.\"\n",
        "context2 = \"I bought a new wireless mouse for my computer.\"\n",
        "\n",
        "sense1 = get_sense(word, context1)\n",
        "sense2 = get_sense(word, context2)\n",
        "\n",
        "print(\"Context 1 Sense:\", sense1)\n",
        "print(\"Definition:\", sense1.definition() if sense1 else \"No definition found\")\n",
        "print()\n",
        "print(\"Context 2 Sense:\", sense2)\n",
        "print(\"Definition:\", sense2.definition() if sense2 else \"No definition found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sViY8-4NmZU",
        "outputId": "55585942-4134-413f-9a83-9a4da2b60744"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context 1 Sense: Synset('shiner.n.01')\n",
            "Definition: a swollen bruise caused by a blow to the eye\n",
            "\n",
            "Context 2 Sense: Synset('mouse.n.04')\n",
            "Definition: a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect langid\n",
        "\n",
        "# Import libraries\n",
        "from langdetect import detect, detect_langs\n",
        "import langid\n",
        "\n",
        "# Example text samples\n",
        "text1 = \"The quick brown fox jumps over the lazy dog.\"  # English\n",
        "text2 = \"El zorro marrón rápido salta sobre el perro perezoso.\"  # Spanish\n",
        "text3 = \"快速的棕色狐狸跳过了懒狗。\"  # Chinese\n",
        "\n",
        "# Using Langdetect\n",
        "print(\"Langdetect:\")\n",
        "print(f\"Text 1: {text1} -> {detect(text1)}\")\n",
        "print(f\"Text 2: {text2} -> {detect(text2)}\")\n",
        "print(f\"Text 3: {text3} -> {detect(text3)}\")\n",
        "\n",
        "# Using Langid\n",
        "print(\"\\nLangid:\")\n",
        "print(f\"Text 1: {text1} -> {langid.classify(text1)[0]}\")\n",
        "print(f\"Text 2: {text2} -> {langid.classify(text2)[0]}\")\n",
        "print(f\"Text 3: {text3} -> {langid.classify(text3)[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9O8VGyuOVqo",
        "outputId": "b135d44c-4a68-422b-9a2a-d7cdb3b06801"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from langid) (1.26.4)\n",
            "Building wheels for collected packages: langdetect, langid\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=edd646d0a496fc3bd767db9527acf1079cd2675ee00d4b886a3c1fed9f584483\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941171 sha256=8040886cbb6ce368456a9f51fec69883a1b0c8aabb544bf8d4b3569900c8fe3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/6a/b6/b7eb43a6ad55b139c15c5daa29f3707659cfa6944d3c696f5b\n",
            "Successfully built langdetect langid\n",
            "Installing collected packages: langid, langdetect\n",
            "Successfully installed langdetect-1.0.9 langid-1.1.6\n",
            "Langdetect:\n",
            "Text 1: The quick brown fox jumps over the lazy dog. -> en\n",
            "Text 2: El zorro marrón rápido salta sobre el perro perezoso. -> es\n",
            "Text 3: 快速的棕色狐狸跳过了懒狗。 -> zh-cn\n",
            "\n",
            "Langid:\n",
            "Text 1: The quick brown fox jumps over the lazy dog. -> en\n",
            "Text 2: El zorro marrón rápido salta sobre el perro perezoso. -> es\n",
            "Text 3: 快速的棕色狐狸跳过了懒狗。 -> zh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Contraction patterns and possessive fixes\n",
        "contraction_patterns = {\n",
        "    \"dont\": \"don't\",\n",
        "    \"cant\": \"can't\",\n",
        "    \"isnt\": \"isn't\",\n",
        "    \"wont\": \"won't\",\n",
        "    \"didnt\": \"didn't\",\n",
        "    \"hasnt\": \"hasn't\",\n",
        "    \"havent\": \"haven't\",\n",
        "    \"doesnt\": \"doesn't\",\n",
        "    \"arent\": \"aren't\",\n",
        "    \"shouldnt\": \"shouldn't\",\n",
        "    \"wouldnt\": \"wouldn't\",\n",
        "    \"couldnt\": \"couldn't\",\n",
        "    \"im\": \"I'm\",\n",
        "    \"youre\": \"you're\",\n",
        "    \"its\": \"it's\",  # Special handling for 'its' vs. 'it's'\n",
        "}\n",
        "\n",
        "def fix_contractions(text):\n",
        "    \"\"\"\n",
        "    Fixes common contractions by adding missing apostrophes.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    corrected_words = []\n",
        "\n",
        "    for word in words:\n",
        "        word_lower = word.lower()\n",
        "        if word_lower in contraction_patterns:\n",
        "            corrected_words.append(contraction_patterns[word_lower])\n",
        "        else:\n",
        "            corrected_words.append(word)\n",
        "\n",
        "    return \" \".join(corrected_words)\n",
        "\n",
        "def fix_possessives(text):\n",
        "    \"\"\"\n",
        "    Fix possessive forms by adding apostrophes where needed.\n",
        "    \"\"\"\n",
        "    # Simple possessive fixes, e.g., dogs -> dog's\n",
        "    text = re.sub(r\"(\\b\\w+)(s)\\b\", r\"\\1's\", text)\n",
        "    return text\n",
        "\n",
        "def correct_apostrophes(text):\n",
        "    \"\"\"\n",
        "    Correct both contractions and possessive forms in the text.\n",
        "    \"\"\"\n",
        "    text = fix_contractions(text)\n",
        "    text = fix_possessives(text)\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "text = \"The dogs didnt know what to do with its ball. She cant believe its happening.\"\n",
        "corrected_text = correct_apostrophes(text)\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Corrected Text:\", corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXf6KxTPPACa",
        "outputId": "811fda56-6bf6-4e83-d115-a8d1da59a255"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The dogs didnt know what to do with its ball. She cant believe its happening.\n",
            "Corrected Text: The dog's didn't know what to do with it's ball. She can't believe it's happening.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordninja\n",
        "import wordninja\n",
        "\n",
        "# Function to segment hashtags\n",
        "def segment_hashtag(hashtag):\n",
        "    # Remove the '#' symbol before segmentation\n",
        "    hashtag_text = hashtag.lstrip('#')\n",
        "\n",
        "    # Use wordninja to split the hashtag into words\n",
        "    segmented = wordninja.split(hashtag_text)\n",
        "\n",
        "    # Rebuild the hashtag with spaces between words\n",
        "    return ' '.join(segmented)\n",
        "\n",
        "# Example usage\n",
        "hashtags = ['#MachineLearningIsAwesome', '#DeepLearningRocks', '#AIRevolution']\n",
        "\n",
        "segmented_hashtags = [segment_hashtag(tag) for tag in hashtags]\n",
        "\n",
        "for original, segmented in zip(hashtags, segmented_hashtags):\n",
        "    print(f\"Original: {original} -> Segmented: {segmented}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcgq0hnyPr-B",
        "outputId": "36ca0b7e-b687-4d45-d280-78113264431c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wordninja\n",
            "  Downloading wordninja-2.0.0.tar.gz (541 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/541.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/541.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.6/541.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-py3-none-any.whl size=541530 sha256=7f41dca4eab33e295bf2a5910b5b89d4529a2e8690985abc3baa2e2be2f5affa\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/66/9c/712044a983337f5d44f90abcd244bd4b8ad28ee64750404b50\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja\n",
            "Successfully installed wordninja-2.0.0\n",
            "Original: #MachineLearningIsAwesome -> Segmented: Machine Learning Is Awesome\n",
            "Original: #DeepLearningRocks -> Segmented: Deep Learning Rocks\n",
            "Original: #AIRevolution -> Segmented: AIR evolution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define a dictionary of acronyms\n",
        "acronym_dict = {\n",
        "    'AI': 'Artificial Intelligence',\n",
        "    'ML': 'Machine Learning',\n",
        "    'NLP': 'Natural Language Processing',\n",
        "    'CNN': 'Convolutional Neural Network',\n",
        "    'RNN': 'Recurrent Neural Network',\n",
        "    'API': 'Application Programming Interface',\n",
        "    'IoT': 'Internet of Things',\n",
        "    'SQL': 'Structured Query Language',\n",
        "    'GPU': 'Graphics Processing Unit',\n",
        "    'CPU': 'Central Processing Unit',\n",
        "    'VPN': 'Virtual Private Network',\n",
        "    'USB': 'Universal Serial Bus',\n",
        "    'HTTP': 'HyperText Transfer Protocol',\n",
        "    'HTTPS': 'HyperText Transfer Protocol Secure',\n",
        "    'JSON': 'JavaScript Object Notation',\n",
        "    'XML': 'Extensible Markup Language'\n",
        "}\n",
        "\n",
        "# Function to expand acronyms with regex\n",
        "def expand_acronyms_regex(text):\n",
        "    def replace(match):\n",
        "        acronym = match.group(0).upper()  # Get matched acronym\n",
        "        return acronym_dict.get(acronym, acronym)  # Replace with expanded form\n",
        "\n",
        "    # Regex to find all uppercase acronyms of length 2 or more\n",
        "    pattern = r'\\b[A-Z]{2,}\\b'\n",
        "    return re.sub(pattern, replace, text)\n",
        "\n",
        "# Example usage\n",
        "input_text = \"I am learning AI and ML. The API for this NLP model is simple.\"\n",
        "expanded_text = expand_acronyms_regex(input_text)\n",
        "\n",
        "print(\"Original Text:\", input_text)\n",
        "print(\"Expanded Text:\", expanded_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_kx3A8uQKF-",
        "outputId": "622106ae-882e-461c-8abe-04a5403e00b1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: I am learning AI and ML. The API for this NLP model is simple.\n",
            "Expanded Text: I am learning Artificial Intelligence and Machine Learning. The Application Programming Interface for this Natural Language Processing model is simple.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the spellchecker library\n",
        "!pip install pyspellchecker\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "def correct_query(query):\n",
        "    spell = SpellChecker()\n",
        "\n",
        "    # Add custom words to prioritize certain corrections\n",
        "    spell.word_frequency.load_words(['weather', 'tomorrow', 'new york'])\n",
        "\n",
        "    words = query.split()\n",
        "\n",
        "    # Correct each word if it's misspelled\n",
        "    corrected_words = []\n",
        "    for word in words:\n",
        "        if word.lower() == \"wether\":  # Explicit rule for this common mistake\n",
        "            corrected_words.append(\"weather\")\n",
        "        else:\n",
        "            corrected_words.append(spell.correction(word) if word in spell.unknown(words) else word)\n",
        "\n",
        "    # Join the corrected words and ensure proper capitalization\n",
        "    corrected_query = ' '.join(corrected_words).capitalize()\n",
        "    return corrected_query\n",
        "\n",
        "query = \"wether tommorrow in new yrok\"\n",
        "corrected_query = correct_query(query)\n",
        "print(corrected_query)\n",
        "# Output: \"Weather tomorrow in new york\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOa17r7HWCEc",
        "outputId": "5ed2ee31-5e9b-428e-96e9-437b9abbc01b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.2)\n",
            "Weather tomorrow in new york\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Example text\n",
        "text = \"Hello! I'm learning Natural Language Processing (NLP), it's amazing. #AI #MachineLearning\"\n",
        "\n",
        "# Function to clean and process the text\n",
        "def process_text(text):\n",
        "    # Remove special characters and numbers (keeping only words and spaces)\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(cleaned_text)\n",
        "\n",
        "    # Convert all tokens to lowercase\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Stem the words\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Process the text\n",
        "processed_text = process_text(text)\n",
        "\n",
        "# Print the results\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Processed Text:\", processed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1fVet3XQuEh",
        "outputId": "9c95c4bb-75fa-47e8-a2d4-eb537dce1309"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Hello! I'm learning Natural Language Processing (NLP), it's amazing. #AI #MachineLearning\n",
            "Processed Text: ['hello', 'im', 'learn', 'natur', 'languag', 'process', 'nlp', 'amaz', 'ai', 'machinelearn']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def resolve_pronouns(text, entities):\n",
        "    # Extract all pronouns and their positions (modified to work with ** for pronouns)\n",
        "    pronoun_pattern = r'\\*\\*(\\w+)\\*\\*'  # Looking for **pronoun**\n",
        "    pronouns = [(match.group(1), match.start()) for match in re.finditer(pronoun_pattern, text)]\n",
        "\n",
        "    # Clean the text by removing ** markers\n",
        "    clean_text = re.sub(r'\\*\\*(\\w+)\\*\\*', r'\\1', text)\n",
        "\n",
        "    # Initialize a list to store the resolved entities\n",
        "    resolved = []\n",
        "\n",
        "    # For each pronoun, find the corresponding entity\n",
        "    for pronoun, pos in pronouns:\n",
        "        closest_entity = None\n",
        "        closest_distance = float('inf')\n",
        "\n",
        "        # Iterate through all entities to find the best match for the pronoun\n",
        "        for entity in entities:\n",
        "            entity_pos = clean_text.rfind(entity, 0, pos)  # Find the last occurrence of the entity before the pronoun\n",
        "            if entity_pos != -1:\n",
        "                distance = pos - (entity_pos + len(entity))\n",
        "                if distance < closest_distance:\n",
        "                    closest_distance = distance\n",
        "                    closest_entity = entity\n",
        "\n",
        "        # Append the resolved entity to the list\n",
        "        resolved.append(closest_entity)\n",
        "\n",
        "    return resolved\n",
        "\n",
        "def main():\n",
        "    # Hardcoded input (replace with your own values)\n",
        "    text_snippet = \"**he** went to the store. **he** bought some milk. **it** was fresh. **he** drank it.\"\n",
        "    entities = [\"John\", \"store\", \"milk\"]\n",
        "\n",
        "    # Resolve pronouns\n",
        "    result = resolve_pronouns(text_snippet, entities)\n",
        "\n",
        "    # Output the resolved entities\n",
        "    for entity in result:\n",
        "        print(entity)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD6i7FGrVf13",
        "outputId": "cac57a98-c123-493c-e786-8723e359f9b6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "store\n",
            "milk\n",
            "milk\n"
          ]
        }
      ]
    }
  ]
}